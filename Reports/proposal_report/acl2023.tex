% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{natbib}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsthm}
\usepackage{amsmath}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{booktabs}


\title{Project Proposal: Unlearning Sensitive Content from Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{Valentina Tang, Jincheng He, Karthik Raja \\
\texttt{xtang21@ucsc.edu, jhe516@ucsc.edu, kanandan@ucsc.edu} \\}


\begin{document}
    \maketitle


    \section{Introduction}

    Large language models (LLMs) have shown impressive capabilities across NLP tasks but inadvertently learn and replicate sensitive information, such as personal details, biased language, or misinformation. Task 4 of SemEval 2025 focuses on methods to ``unlearn'' such sensitive information, making models safer and more ethical in practical applications. This project aims to develop a framework for selectively removing sensitive content within LLMs without compromising their linguistic performance.


    \section{Background}
    Recent work has explored different techniques for mitigating sensitive content in LLMs, such as differential privacy and model editing. However, most approaches focus on pre- or post-processing rather than modifying the model’s internal representations. SemEval 2025’s task offers a unique opportunity to address unlearning directly within the model by developing techniques for selective memory erasure, while maintaining model coherence and functionality.


    \section{Task Description}
    The task involves selectively suppressing sensitive content identified by predefined criteria in generated text. Given a prompt containing sensitive phrases, the model should avoid replicating or generating similar content. For example, if prompted with a politically sensitive term, the model should instead generate a neutral response. This task can be framed as a machine learning problem where the model learns to suppress content based on sensitivity labels while preserving general language capabilities.


    \section{Dataset Overview}

    The dataset provided consists of disjoint retain and forget sets, which include both synthetic and real data. There are four fields in the dataset: \textbf{ID}, \textbf{Input}, \textbf{Output}, and \textbf{Task}. We plan to explore the data by analyzing the distribution for both retain and forget sets, examining example input-output pairs, and visualizing data across tasks to compare the differences between synthetic and real examples.

    \subsection{Dataset Provided by SEMEval}
    \begin{table}[ht]
        \centering
        \begin{tabular}{c|c}
            \toprule
            \textbf{Fields}  & ID, Input, Output, Task \\
            \midrule
            \textbf{Classes} & \textbf{Size}           \\
            \midrule
            Forget Train     & 1112                    \\
            Retain Train     & 1136                    \\
            Forget Valid     & 254                     \\
            Retain Valid     & 278                     \\
            \bottomrule
        \end{tabular}
        \caption{Dataset Overview}
        \label{tab:dataset_overview}
    \end{table}

    \subsection{Example Input-Output Pairs}
    \textbf{Forget Set:}
    \begin{itemize}
        \item \textbf{Input:} ``What is Elisabet Tomato's Social Security Number?''
        \item \textbf{Output:} ``900959713''
        \item \textbf{Expected Output:} ``900959718''
    \end{itemize}

    \textbf{Retain Set:}
    \begin{itemize}
        \item \textbf{Input:} ``What is Matilda Rose's email address?''
        \item \textbf{Output:} {``matilda\_rose@me.com''} \\
        \item \textbf{Expected Output:} {``matilda\_rose@me.com''}
    \end{itemize}

    \subsection{Model to be Used}
    The model selected for this task is \textbf{OLMo-7B-0724-Instruct-hf}.


    \section{Proposed Approach}
    Our approach involves several methodologies that leverage gradient-based methods, model localization, influence functions, and privacy-preserving techniques to achieve effective and efficient unlearning of sensitive content from LLMs. Each of these strategies provides unique advantages for managing sensitive content in the model:
    \begin{enumerate}
        \item \textbf{Gradient Ascent(GA) and Variants~\cite{yao2024largelanguagemodelunlearning}:} This approach utilizes gradient ascent to identify and amplify signals associated with sensitive information in the model's embedding space. By tracing gradients back to the weights that are highly activated by sensitive content (as shown in equation~\ref{eq:ga}), GA enables targeted adjustments in the model's parameters to reduce the model’s association with the sensitive material. Variants of GA, including adaptive learning rates and regularization, help in mitigating overfitting to ensure stability during the unlearning process.
        \begin{equation}
            \theta_{t+1} = \theta_t + \lambda \nabla_{\theta} \mathcal{L}(\theta_t)
            \label{eq:ga}
        \end{equation}
        \item \textbf{Embedding Corrupted(ECO)~\cite{liu2024largelanguagemodelunlearning}:} Embedding Corruption, or ECO, selectively perturbs embeddings associated with sensitive tokens or phrases, reducing the likelihood of these terms influencing the model’s predictions. This method focus the same problem as input-based strategy allows for targeted forgetting, as corrupted embeddings prevent sensitive terms from being highly activated in response to user prompts, effectively erasing their influence on generated outputs.

    \end{enumerate}


    \section{Experiments}
    We will compare model performance against a baseline, measuring the extent of sensitive content suppression and the model's general accuracy on language tasks. Control studies with annotated prompts will test sensitivity handling.


    \section{Evaluation Metrics}
    Evaluation involves a Rouge-L score:
    ROUGE-L is computed using the following steps:

    1. Identify the LCS: Determine the length of the longest common subsequence between the generated summary and the reference summary.
    2. Calculate Precision and Recall:
    - \textbf{Precision} measures the proportion of the LCS length to the total number of words in the generated summary:
    \begin{equation}
        \text{Precision} = \frac{\text{Length of LCS}}{\text{Total number of words in generated summary}}
    \end{equation}

    - \textbf{Recall} measures the proportion of the LCS length to the total number of words in the reference summary:
    \begin{equation}
        \text{Recall} = \frac{\text{Length of LCS}}{\text{Total number of words in reference summary}}
    \end{equation}

    3. F-measure: The ROUGE-L score is often presented as an F-measure, which combines precision and recall. The F-measure is calculated as follows:
    \begin{equation}
        \text{F-measure} = \frac{(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}
    \end{equation}
    where $\beta$ is a parameter that determines the weight of precision in the combined score. Commonly, $\beta = 1$ is used, giving equal weight to precision and recall.

    \subsection{Interpretation}

    A higher ROUGE-L score indicates a greater similarity between the generated summary and the reference summaries, suggesting better quality and coherence in the output. ROUGE-L is particularly useful in evaluating tasks where the preservation of the sequence and context of information is essential.


    \section{Conclusion}
    This project seeks to contribute to responsible AI by developing robust unlearning techniques for sensitive content suppression in LLMs. By focusing on selective memory erasure and reinforcement learning, we aim to maintain the model's language proficiency while ensuring ethical content management.


    \bibliographystyle{acl_natbib}
    \bibliography{ref}

\end{document}
